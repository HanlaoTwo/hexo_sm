---
title: 5.spark程序提交集群和监控
date: 2018-07-10 21:09:19
tags: spark
---
### 运行模式
Spark 应用在集群上作为独立的进程组来运行，在 main 程序中通过 SparkContext 来协调（称之为 driver 程序）  

![image](http://spark.apache.org/docs/latest/img/cluster-overview.png)

1.每个应用获取到它自己的 Executor 进程，它们会保持在整个应用的生命周期中并且在多个线程中运行 Task（任务）。这样做的优点是把应用互相隔离，在调度方面（每个 driver 调度它自己的 task）和 Executor 方面（来自不同应用的 task 运行在不同的 JVM 中）。然而，这也意味着若是不把数据写到外部的存储系统中的话，数据就不能够被不同的 Spark 应用（SparkContext 的实例）之间共享。

2.Spark 是不知道底层的 Cluster Manager 到底是什么类型的。只要它能够获得 Executor 进程，并且它们可以和彼此之间通信，那么即使是在一个也支持其它应用的 Cluster Manager（例如，Mesos / YARN）上来运行它也是相对简单的。

3.Driver 程序必须在自己的生命周期内（例如，请看 在网络中配置 spark.driver.port 章节）监听和接受来自它的 Executor 的连接请求。同样的，driver 程序必须可以从 worker 节点上网络寻址（就是网络没问题）。

4.因为 driver 调度了集群上的 task（任务），更好的方式应该是在相同的局域网中靠近 worker 的节点上运行。如果您不喜欢发送请求到远程的集群，倒不如打开一个 RPC 至 driver 并让它就近提交操作而不是从很远的 worker 节点上运行一个 driver。
### 集群类型
1.Standalone – 包含在 Spark 中使得它更容易来安装集群的一个简单的 Cluster Manager。

2.Mesos – 一个通用的 Cluster Manager，它也可以运行 Hadoop MapReduce 和其它服务应用。

3.Hadoop YARN – Hadoop 2 中的 resource manager（资源管理器）。

### Standalone
#### 部署
启动集群：

spark根目录下

    ./sbin/start-master.sh
    
启动之后可以在ip:8080上访问集群信息

运行程序
spark submit命令可以启动程序

    spark-submit --class SparkSQLExample --master spark://ip:port /root/worspace/test-1.0.jar
#### 监控
Spark 的独立模式提供了一个基于 Web 的用户界面来监控集群。Master 和 每个 Worker 都有自己的 web 用户界面，显示集群和作业统计信息。默认情况下你可以在 8080 端口访问 Web UI 的主端口也可以在配置文件中或通过命令行选项进行更改。

在程序运行的时候可以在ip:4040监控运行状态

### Hadoop YARN 

#### 部署

配置HADOOP_CONF_DIR
```
vim /etc/profile 
#添加
HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop  
cd $SPARK_HOME/conf
vim spark-env.sh 
#添加
HADOOP_CONF_DIR=$HADOOP_CONF_DIR
#启动hadoop
cd $HADOOP_HOME/sbin  
./start-all.sh
#新建日志目录
mkdir /root/logs
#启动历史信息记录服务
cd $SPARK_HOME/sbin
./sbin/start-history-server.sh /root/logs
#启动应用
./bin/spark-submit --class SparkSQLExample --master yarn --deploy-mode cluster  /root/worspace/test-1.0.jar
#如果有参数，参数放在最后
```
#### 监控
配置日志目录
```
cd $SPARK_HIME/conf
vim spark-defaults.conf
#添加
spark.eventLog.enabled           true
 spark.eventLog.dir               /root/logs/spark
 spark.history.fs.logDirectory    /root/logs/spark
```
日志目录可以是本地目录也可以hdfs目录

监控地址
运行时：

ip:4040(spark)

ip:8088（hadoop）

ip:18080(spark历史)