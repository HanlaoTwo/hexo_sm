---
title: 9.JavaScript，Scala，spark中的闭包
date: 2018-07-10 21:09:19
tags: spark
---
#### JavaScript的闭包
```
(function(){
    var hello="hello,world";
    function welcome(hi){
        alert(hi);        //解析到作用域链的第一个对象的属性
        alert(hello);    //解析到作用域链的第二个对象的属性
    }
    welcome("It's easy");
})();

作者：知乎用户
链接：https://www.zhihu.com/question/34547104/answer/59515735
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
```
每次定义一个函数，都会产生一个作用域链（scope chain）。当JavaScript寻找变量varible时（这个过程称为变量解析），总会优先在当前作用域链的第一个对象中查找属性varible，如果找到，则直接使用这个属性；否则，继续查找下一个对象的是否存在这个属性；这个过程会持续直至找到这个属性或者最终未找到引发错误为止

#### scala闭包
```
object Closure {
  def main(args: Array[String]): Unit = {
    val c = add(1)
    println(c)
  }

  val b = 10
  val add = (a: Int) => {
    b - a
  }
}
```
闭包是一个函数，它返回值取决于在此函数之外声明的一个或多个变量的值。

#### spark中的闭包

在集群中执行代码时，一个关于 Spark 更难的事情是理解的变量和方法的范围和生命周期。

修改其范围之外的变量 RDD 操作可以混淆的常见原因。在下面的例子中，我们将看一下使用的 foreach() 代码递增累加计数器，但类似的问题，也可能会出现其他操作上。

示例
考虑一个简单的 RDD 元素求和，以下行为可能不同，具体取决于是否在同一个 JVM 中执行。
一个常见的例子是当 Spark 运行在本地模式（--master = local[n]）时，与部署 Spark 应用到群集（例如，通过 spark-submit 到 YARN）: 
```
var counter = 0
var rdd = sc.parallelize(data)
 
// Wrong: Don't do this!!
rdd.foreach(x => counter += x)
 
println("Counter value: " + counter)
```